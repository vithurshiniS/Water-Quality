# -*- coding: utf-8 -*-
"""STP_test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kWib6_KJu4mq8yTqrx7nw6T10OCV1x_t
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler

# from google.colab import drive
# drive.mount('/content/drive')

new_csv_file_path = 'Vithu_SWTP.csv'

# Load the updated data from the new CSV file
new_data = pd.read_csv(new_csv_file_path)

# Display the first few rows of the dataset and its summary
new_data.head(), new_data.describe()

from sklearn.preprocessing import MinMaxScaler
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats

# Extract the data and remove any initial rows containing NaN
df = new_data.dropna()

# Selecting relevant columns for normalization
columns_to_normalize = ['RW_Turbidity', 'pH', 'Conductivity', 'PAC']

# Normalize the dataset values between 0 and 1 using MinMaxScaler
scaler = MinMaxScaler()
normalized_data = pd.DataFrame(scaler.fit_transform(df[columns_to_normalize]), columns=columns_to_normalize)

# Create QQ plots to identify outliers
def plot_qq(data, feature_name):
    plt.figure(figsize=(6, 6))
    stats.probplot(data, dist="norm", plot=plt)
    plt.title(f'QQ Plot for {feature_name}')
    plt.grid(True)
    plt.show()

# Plot QQ plots for all input features
for col in columns_to_normalize:
    plot_qq(normalized_data[col], col)

# Display the first few rows of the normalized data
normalized_data.head()

# Remove outliers using the z-score method
z_scores = np.abs(stats.zscore(normalized_data))
threshold = 3
outliers_removed_data = normalized_data[(z_scores < threshold).all(axis=1)]

# Check how much data remains after removing outliers
remaining_data_info = {
    "Original Count": len(normalized_data),
    "Remaining Count": len(outliers_removed_data),
    "Removed Count": len(normalized_data) - len(outliers_removed_data)
}

remaining_data_info, outliers_removed_data.head()

from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestRegressor

# Separate features (X) and target (y)
X = outliers_removed_data[['RW_Turbidity', 'pH', 'Conductivity']]
y = outliers_removed_data['PAC']

# Initialize the Random Forest Regressor model
model = RandomForestRegressor(n_estimators=100, random_state=42)

# Perform Recursive Feature Elimination (RFE) using the Random Forest Regressor estimator
selector = RFE(model, n_features_to_select=2, step=1)
selector = selector.fit(X, y)

# Find the selected features
selected_features_mask = selector.support_
selected_features = X.columns[selected_features_mask]

# Get a summary of the selected features
selected_features_summary = selected_features.tolist()

# Output the selected features and the ranking
print("Selected Features:", selected_features_summary)
print("Feature Ranking:", selector.ranking_)

from sklearn.model_selection import train_test_split

# Use the selected features only
X_selected = outliers_removed_data[['RW_Turbidity', 'Conductivity' ]]
y_selected = outliers_removed_data['PAC']

# Split data into training, validation, and testing sets (60% train, 20% validation, 20% test)
X_train, X_temp, y_train, y_temp = train_test_split(X_selected, y_selected, test_size=0.4, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Check data division proportions
train_val_test_counts = {
    "Training Count": len(X_train),
    "Validation Count": len(X_val),
    "Testing Count": len(X_test)
}

train_val_test_counts

print (y_selected)

from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
import numpy as np
from sklearn.base import BaseEstimator, RegressorMixin

# GRNN Approximation using KNN
knn_grnn = make_pipeline(StandardScaler(), KNeighborsRegressor(n_neighbors=5))

# Train the KNN "GRNN" model
knn_grnn.fit(X_train, y_train)

# Validate the KNN "GRNN" model
y_val_pred = knn_grnn.predict(X_val)
grnn_val_mse = mean_squared_error(y_val, y_val_pred)

# Define an ELM-RBF-like model using a custom approximation
class ExtremeLearningMachineRBF(BaseEstimator, RegressorMixin):
    def __init__(self, hidden_neurons=10):
        self.hidden_neurons = hidden_neurons
        self.centers = None
        self.weights = None

    def _rbf(self, x, center, sigma):
        return np.exp(-np.linalg.norm(x - center) ** 2 / (2 * sigma ** 2))

    def _rbf_layer(self, X, centers, sigma):
        G = np.zeros((X.shape[0], centers.shape[0]))
        for i, c in enumerate(centers):
            for j, x in enumerate(X):
                G[j, i] = self._rbf(x, c, sigma)
        return G

    def fit(self, X, y):
        # Randomly select centers from the input data
        indices = np.random.choice(X.shape[0], self.hidden_neurons, replace=False)
        self.centers = X[indices]
        sigma = np.mean(np.linalg.norm(X - self.centers[:, None], axis=2))

        # Calculate the RBF layer output
        G = self._rbf_layer(X, self.centers, sigma)

        # Compute the output weights
        self.weights = np.linalg.pinv(G).dot(y)

    def predict(self, X):
        # Calculate the RBF layer output
        sigma = np.mean(np.linalg.norm(X - self.centers[:, None], axis=2))
        G = self._rbf_layer(X, self.centers, sigma)
        return G.dot(self.weights)

# Train the ELM-RBF model
elm_rbf_model = ExtremeLearningMachineRBF(hidden_neurons=10)
elm_rbf_model.fit(X_train.to_numpy(), y_train.to_numpy())

# Validate the ELM-RBF model
y_val_elm_pred = elm_rbf_model.predict(X_val.to_numpy())
elm_val_mse = mean_squared_error(y_val, y_val_elm_pred)

# Compare validation MSE for both models
model_performance = {
    "GRNN (KNN Approximation) Validation MSE": grnn_val_mse,
    "ELM-RBF Validation MSE": elm_val_mse
}

model_performance

# Evaluate the KNN "GRNN" model on the testing set
y_test_pred = knn_grnn.predict(X_test)
grnn_test_mse = mean_squared_error(y_test, y_test_pred)

# Evaluate the ELM-RBF model on the testing set
y_test_elm_pred = elm_rbf_model.predict(X_test.to_numpy())
elm_test_mse = mean_squared_error(y_test, y_test_elm_pred)

# Compare test MSE for both models
test_performance = {
    "GRNN (KNN Approximation) Test MSE": grnn_test_mse,
    "ELM-RBF Test MSE": elm_test_mse
}

test_performance

from sklearn.metrics import r2_score

# Calculate R-squared (R^2) values for both models on the testing set
grnn_test_r2 = r2_score(y_test, y_test_pred)
elm_test_r2 = r2_score(y_test, y_test_elm_pred)

# Compare the R values
r_values = {
    "GRNN (KNN Approximation) Test R^2": grnn_test_r2,
    "ELM-RBF Test R^2": elm_test_r2
}

r_values



import tensorflow as tf
from tensorflow.keras import layers, models

# Reshape data to fit a 2D CNN input format (samples, height, width, channels)
# Since we have only 2 features, we'll use (samples, 2, 1, 1)
X_train_cnn = X_train.to_numpy().reshape(-1, 2, 1, 1)
X_val_cnn = X_val.to_numpy().reshape(-1, 2, 1, 1)
X_test_cnn = X_test.to_numpy().reshape(-1, 2, 1, 1)

# Define the CNN model architecture
cnn_model = models.Sequential([
    layers.Conv2D(32, (1, 1), activation='relu', input_shape=(2, 1, 1)),
    layers.MaxPooling2D((1, 1)),
    layers.Flatten(),
    layers.Dense(32, activation='relu'),
    layers.Dense(16, activation='relu'),
    layers.Dense(1)
])

# Compile the model
cnn_model.compile(optimizer='adam', loss='mean_squared_error')

# Train the CNN model
cnn_model.fit(X_train_cnn, y_train, epochs=100, validation_data=(X_val_cnn, y_val), verbose=0)

# Validate the CNN model
y_val_cnn_pred = cnn_model.predict(X_val_cnn).flatten()
cnn_val_mse = mean_squared_error(y_val, y_val_cnn_pred)

# Evaluate the CNN model on the testing set
y_test_cnn_pred = cnn_model.predict(X_test_cnn).flatten()
cnn_test_mse = mean_squared_error(y_test, y_test_cnn_pred)
cnn_test_r2 = r2_score(y_test, y_test_cnn_pred)

# Output the CNN results
cnn_results = {
    "CNN Validation MSE": cnn_val_mse,
    "CNN Test MSE": cnn_test_mse,
    "CNN Test R^2": cnn_test_r2
}

cnn_results

# Reshape data to fit a 2D LSTM input format (samples, time steps, features)
# Since we have only 2 features, we'll use (samples, time steps, 2)
X_train_lstm = X_train.to_numpy().reshape(-1, 1, 2)
X_val_lstm = X_val.to_numpy().reshape(-1, 1, 2)
X_test_lstm = X_test.to_numpy().reshape(-1, 1, 2)

# Define the LSTM model architecture
lstm_model = models.Sequential([
    layers.LSTM(32, input_shape=(1, 2), activation='relu', return_sequences=True),
    layers.LSTM(16, activation='relu'),
    layers.Dense(1)
])

# Compile the model
lstm_model.compile(optimizer='adam', loss='mean_squared_error')

# Train the LSTM model
lstm_model.fit(X_train_lstm, y_train, epochs=100, validation_data=(X_val_lstm, y_val), verbose=0)

# Validate the LSTM model
y_val_lstm_pred = lstm_model.predict(X_val_lstm).flatten()
lstm_val_mse = mean_squared_error(y_val, y_val_lstm_pred)

# Evaluate the LSTM model on the testing set
y_test_lstm_pred = lstm_model.predict(X_test_lstm).flatten()
lstm_test_mse = mean_squared_error(y_test, y_test_lstm_pred)
lstm_test_r2 = r2_score(y_test, y_test_lstm_pred)

# Output the LSTM results
lstm_results = {
    "LSTM Validation MSE": lstm_val_mse,
    "LSTM Test MSE": lstm_test_mse,
    "LSTM Test R^2": lstm_test_r2
}

lstm_results

# Define the autoencoder model architecture
autoencoder_model = models.Sequential([
    layers.Dense(16, activation='relu', input_shape=(2,)),
    layers.Dense(8, activation='relu'),
    layers.Dense(4, activation='relu'),
    layers.Dense(8, activation='relu'),
    layers.Dense(16, activation='relu'),
    layers.Dense(2)  # Compressed representation
])

# Compile the autoencoder model
autoencoder_model.compile(optimizer='adam', loss='mean_squared_error')

# Train the autoencoder model
autoencoder_model.fit(X_train, X_train, epochs=100, validation_data=(X_val, X_val), verbose=0)

# Extract compressed features using the trained autoencoder
encoder_model = models.Sequential(autoencoder_model.layers[:3])

# Extract features for training, validation, and testing sets
X_train_encoded = encoder_model.predict(X_train)
X_val_encoded = encoder_model.predict(X_val)
X_test_encoded = encoder_model.predict(X_test)

# Define the regression model architecture using the extracted features
regression_model = models.Sequential([
    layers.Dense(32, activation='relu', input_shape=(X_train_encoded.shape[1],)),
    layers.Dense(16, activation='relu'),
    layers.Dense(1)  # Output layer
])

# Compile the regression model
regression_model.compile(optimizer='adam', loss='mean_squared_error')

# Train the regression model
regression_model.fit(X_train_encoded, y_train, epochs=100, validation_data=(X_val_encoded, y_val), verbose=0)

# Evaluate the regression model on the testing set
y_test_pred = regression_model.predict(X_test_encoded).flatten()
test_r2 = r2_score(y_test, y_test_pred)

test_r2

#Build and train the autoencoder
autoencoder_model = models.Sequential([
    layers.Dense(16, activation='relu', input_shape=(X_train.shape[1],)),
    layers.Dense(8, activation='relu'),
    layers.Dense(4, activation='relu'),  # Bottleneck/compressed representation
    layers.Dense(8, activation='relu'),
    layers.Dense(16, activation='relu'),
    layers.Dense(X_train.shape[1], activation=None)  # Reconstruction
])

autoencoder_model.compile(optimizer='adam', loss='mean_squared_error')
autoencoder_model.fit(X_train, X_train, epochs=50, validation_data=(X_val, X_val), verbose=0)

# Extract compressed features
encoder_model = models.Sequential(autoencoder_model.layers[:3])
X_train_encoded = encoder_model.predict(X_train)
X_val_encoded = encoder_model.predict(X_val)
X_test_encoded = encoder_model.predict(X_test)

# Train and evaluate the General Regression Neural Network (GRNN)
# Note: We'll use an approximate GRNN implementation using sklearn's KNeighborsRegressor.
from sklearn.neighbors import KNeighborsRegressor

grnn_model = KNeighborsRegressor(n_neighbors=1, weights='distance')  # Approximation of GRNN
grnn_model.fit(X_train_encoded, y_train)

y_test_pred_grnn = grnn_model.predict(X_test_encoded)
grnn_test_r2 = r2_score(y_test, y_test_pred_grnn)

# Train and evaluate the Extreme Learning Machine (ELM) with Radial Basis Function (RBF)
from sklearn.kernel_ridge import KernelRidge

elm_rbf_model = KernelRidge(kernel='rbf')
elm_rbf_model.fit(X_train_encoded, y_train)

y_test_pred_elm_rbf = elm_rbf_model.predict(X_test_encoded)
elm_rbf_test_r2 = r2_score(y_test, y_test_pred_elm_rbf)

# Output the results
results = {
    "GRNN Test R2": grnn_test_r2,
    "ELM-RBF Test R2": elm_rbf_test_r2
}

grnn_test_r2
elm_rbf_test_r2

import joblib

# Train the ELM-RBF model (if not already done)
elm_rbf_model = ExtremeLearningMachineRBF(hidden_neurons=10)
elm_rbf_model.fit(X_train.to_numpy(), y_train.to_numpy())

# Save the trained model
joblib_file = "elm_rbf_model.pkl"
joblib.dump(elm_rbf_model, joblib_file)

import streamlit as st
import numpy as np
import joblib

# Load the trained model
model = joblib.load('elm_rbf_model.pkl')

# Streamlit UI
st.title('PAC Prediction from Water Quality Parameters')

# User inputs
rw_turbidity = st.number_input('Enter RW Turbidity:', min_value=0.0, format="%.5f")
conductivity = st.number_input('Enter Conductivity:', min_value=0.0, format="%.5f")

# Prepare the input data
input_data = np.array([[rw_turbidity, conductivity]])

# Predict PAC
if st.button('Predict PAC'):
    pac_prediction = model.predict(input_data)
    st.write(f'Predicted PAC: {pac_prediction[0]:.5f}')
    
    
import streamlit as st
import numpy as np
import joblib

# Load the trained models
elm_rbf_model = joblib.load('elm_rbf_model.pkl')
knn_grnn_model = joblib.load('knn_grnn_model.pkl')

# Streamlit UI
st.title('PAC Prediction from Water Quality Parameters')

# User inputs
rw_turbidity = st.number_input('Enter RW Turbidity:', min_value=0.0, format="%.5f")
conductivity = st.number_input('Enter Conductivity:', min_value=0.0, format="%.5f")

# Model selection
model_choice = st.selectbox('Choose the model', ('ELM-RBF', 'GRNN (KNN Approximation)'))

# Prepare the input data
input_data = np.array([[rw_turbidity, conductivity]])

# Predict PAC
if st.button('Predict PAC'):
    if model_choice == 'ELM-RBF':
        model = elm_rbf_model
    else:
        model = knn_grnn_model

    pac_prediction = model.predict(input_data)
    st.write(f'Predicted PAC: {pac_prediction[0]:.5f}')
